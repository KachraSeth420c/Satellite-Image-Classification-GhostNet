{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ipgMI6qxHT9O",
    "outputId": "7e0ab582-1cb8-408c-ac35-62c7492e203a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPDgaQ4D2oLV"
   },
   "source": [
    "**We have commented those libraries which you dont need if you skip the augmentation of images part**\n",
    "\n",
    "We are already giving that data in GNR folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ugEdP-0AHZfw"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import cv2\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# For model building\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision.datasets import ImageFolder\n",
    "# from torchvision.utils import make_grid\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "# import PIL\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "# from tensorflow.keras.models import Sequential\n",
    "import random\n",
    "# import tensorflow.keras.layers as Layers\n",
    "# import tensorflow.keras.activations as Actications\n",
    "# import tensorflow.keras.models as Models\n",
    "# import tensorflow.keras.optimizers as Optimizer\n",
    "import os\n",
    "import shutil\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# import tensorflow.keras.metrics as Metrics\n",
    "# import tensorflow.keras.utils as Utils\n",
    "# from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6pflOwXwtCMq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqz5rF0RJAw3"
   },
   "source": [
    "**Dont run below codes, We already gave you the augmented train data and validation data**\n",
    "\n",
    "You can save time by skipping over these data augmentation codes\n",
    "\n",
    "Run skipping 3 cell blocks (containing augmenting code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35fiLkbTs-5Y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xe8SyJMFs--V"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tpQmLrtALDWX"
   },
   "source": [
    "**If u decide to run the below augmentating images code, then delete the 'new_train' folder in the given GNR folder because these codes are going to generate it. If u don't, then they will store new generated images in that old old 'new_train' folder and the size of training dataset will double.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UROe21IgL1l5"
   },
   "source": [
    "Already it was taking much time in executing it for 3 epoch on training dataset due to system constrainst :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YX2Ov0iDL0wp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0ZfrGauHZln"
   },
   "outputs": [],
   "source": [
    "# Augmenting all Train folder images\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from tqdm import tqdm\n",
    "datagen = ImageDataGenerator(\n",
    "        fill_mode = \"constant\",\n",
    "        rotation_range=5,\n",
    "        shear_range=0.05,\n",
    "        width_shift_range=0.01,\n",
    "        zoom_range=0.1,\n",
    "        height_shift_range=0.01,\n",
    "        vertical_flip=True,\n",
    "        horizontal_flip=True)\n",
    "classes = ['golf_course','crosswalk', 'bridge', 'basketball_court']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O1a8m-56HZo3"
   },
   "outputs": [],
   "source": [
    "# Generating 12 augmented images from single train image\n",
    "for cls in classes:\n",
    "    for filename in tqdm(os.listdir(\"/content/drive/My Drive/GNR/dataset/train/\"+cls)):\n",
    "        img = load_img(\"/content/drive/My Drive/GNR/dataset/train/\"+cls+\"/\"+filename)  # this is a PIL image\n",
    "        x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "        x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "\n",
    "        i = 0\n",
    "        for batch in datagen.flow(x, batch_size=1,\n",
    "                              save_to_dir=\"/content/drive/My Drive/GNR/dataset/train_new/train/\"+cls, save_prefix=filename, save_format='jpeg'):\n",
    "            i += 1\n",
    "            if i > 11:\n",
    "                break  # otherwise the generator would loop indefinitely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNfmebNTHZrs"
   },
   "outputs": [],
   "source": [
    "#  Creating Train / Val / Test folders (One time use)\n",
    "\n",
    "root_dir = \"/content/drive/My Drive/GNR/dataset/train\"\n",
    "classes_dir = ['golf_course','crosswalk', 'bridge', 'basketball_court']\n",
    "\n",
    "val_ratio = 0.20\n",
    "\n",
    "for cls in classes_dir:\n",
    "    os.makedirs(root_dir +'/train/' + cls)\n",
    "    os.makedirs(root_dir +'/val/' + cls)\n",
    "\n",
    "\n",
    "    # Creating partitions of the data after shuffeling\n",
    "    src = root_dir + \"/\" + cls # Folder to copy images from\n",
    "\n",
    "    allFileNames = os.listdir(src)\n",
    "    np.random.shuffle(allFileNames)\n",
    "    train_FileNames, val_FileNames= np.split(np.array(allFileNames),\n",
    "                                                              [int(len(allFileNames)* (1 - val_ratio))])\n",
    "\n",
    "\n",
    "    # train_FileNames = [src+'/'+ name for name in train_FileNames.tolist()]\n",
    "    val_FileNames = [src+'/' + name for name in val_FileNames.tolist()]\n",
    "\n",
    "    print('Total images: ', len(allFileNames))\n",
    "    # print('Training: ', len(train_FileNames))\n",
    "    print('Validation: ', len(val_FileNames))\n",
    "\n",
    "    # Copy-pasting images\n",
    "    # for name in train_FileNames:\n",
    "    #     shutil.copy(name, root_dir +'/train/' + cls)\n",
    "\n",
    "    for name in val_FileNames:\n",
    "        shutil.copy(name, root_dir +'/val/' + cls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52MdQ_A3iyrk"
   },
   "source": [
    "**Now you can run from below to test our approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lv8tpC5JSeG"
   },
   "source": [
    "**Make sure mounting is correct**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nTrLuaRIHZuA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gK8D7EFLHZwa"
   },
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        #transforms.RandomCrop(224),\n",
    "        transforms.Resize((256,256)),\n",
    "        #transforms.RandomResizedCrop(512,scale=(0.7, 1.0), ratio=(0.75, 1.3333333333333333)),\n",
    "        #transforms.RandomHorizontalFlip(p=0.5),\n",
    "        #transforms.RandomVerticalFlip(p=0.5),\n",
    "        # transforms.RandomAffine(5, translate=(0.2,0.2), scale=(0.9,1.2), shear=(10,10), resample=False, fillcolor=0),\n",
    "        # transforms.RandomPerspective(distortion_scale=0.1),\n",
    "        #transforms.RandomRotation(10, resample=False, expand=False),\n",
    "        #transforms.RandomErasing(p=0.5),\n",
    "        #transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([0.3794, 0.3961, 0.3725], [0.1846, 0.1853, 0.1804])\n",
    "    ]),\n",
    "    'validation': transforms.Compose([                                    \n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([0.3794, 0.3961, 0.3725], [0.1846, 0.1853, 0.1804])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((256,256)), \n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([0.3794, 0.3961, 0.3725], [0.1846, 0.1853, 0.1804])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WBjgSJrgHolY",
    "outputId": "bbc7f7e9-1325-4469-e6ac-fb96a4e67833"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "#making the dataset\n",
    "train_dataset = datasets.ImageFolder(\"/content/drive/My Drive/GNR/dataset/train_new/train/\", data_transforms['train'])\n",
    "val_dataset = datasets.ImageFolder(\"/content/drive/My Drive/GNR/dataset/train/val/\", data_transforms['validation'])\n",
    "\n",
    "\n",
    "#making the dataloader\n",
    "dataloaders = {}\n",
    "dataloaders['train'] = torch.utils.data.DataLoader(\n",
    "                                    train_dataset,                                        \n",
    "                                    batch_size=100, \n",
    "                                    shuffle=True, num_workers=4) # crashes for batch size >120\n",
    "dataloaders['validation'] = torch.utils.data.DataLoader(\n",
    "                                    val_dataset,                                        \n",
    "                                    batch_size=10,\n",
    "                                    shuffle=True, num_workers=4)\n",
    "\n",
    "dataset_sizes = {}\n",
    "dataset_sizes['train'] = len(train_dataset)\n",
    "dataset_sizes['validation'] = len(val_dataset)\n",
    "classes = train_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "caYp8nUmHonv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ao2ZY1HrHoqJ"
   },
   "outputs": [],
   "source": [
    "# print(dataset_sizes['train'])\n",
    "# print(dataloaders['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "RIZjJJE1HZyj"
   },
   "outputs": [],
   "source": [
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "def hard_sigmoid(x, inplace: bool = False):\n",
    "    if inplace:\n",
    "        return x.add_(3.).clamp_(0., 6.).div_(6.)\n",
    "    else:\n",
    "        return F.relu6(x + 3.) / 6.\n",
    "\n",
    "\n",
    "class SqueezeExcite(nn.Module):\n",
    "    def __init__(self, in_chs, se_ratio=0.25, reduced_base_chs=None,\n",
    "                 act_layer=nn.ReLU, gate_fn=hard_sigmoid, divisor=4, **_):\n",
    "        super(SqueezeExcite, self).__init__()\n",
    "        self.gate_fn = gate_fn\n",
    "        reduced_chs = _make_divisible((reduced_base_chs or in_chs) * se_ratio, divisor)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv_reduce = nn.Conv2d(in_chs, reduced_chs, 1, bias=True)\n",
    "        self.act1 = act_layer(inplace=True)\n",
    "        self.conv_expand = nn.Conv2d(reduced_chs, in_chs, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_se = self.avg_pool(x)\n",
    "        x_se = self.conv_reduce(x_se)\n",
    "        x_se = self.act1(x_se)\n",
    "        x_se = self.conv_expand(x_se)\n",
    "        x = x * self.gate_fn(x_se)\n",
    "        return x    \n",
    "\n",
    "    \n",
    "class ConvBnAct(nn.Module):\n",
    "    def __init__(self, in_chs, out_chs, kernel_size,\n",
    "                 stride=1, act_layer=nn.ReLU):\n",
    "        super(ConvBnAct, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_chs, out_chs, kernel_size, stride, kernel_size//2, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_chs)\n",
    "        self.act1 = act_layer(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GhostModule(nn.Module):\n",
    "    def __init__(self, inp, oup, kernel_size=1, ratio=2, dw_size=3, stride=1, relu=True):\n",
    "        super(GhostModule, self).__init__()\n",
    "        self.oup = oup\n",
    "        init_channels = math.ceil(oup / ratio)\n",
    "        new_channels = init_channels*(ratio-1)\n",
    "\n",
    "        self.primary_conv = nn.Sequential(\n",
    "            nn.Conv2d(inp, init_channels, kernel_size, stride, kernel_size//2, bias=False),\n",
    "            nn.BatchNorm2d(init_channels),\n",
    "            nn.ReLU(inplace=True) if relu else nn.Sequential(),\n",
    "        )\n",
    "\n",
    "        self.cheap_operation = nn.Sequential(\n",
    "            nn.Conv2d(init_channels, new_channels, dw_size, 1, dw_size//2, groups=init_channels, bias=False),\n",
    "            nn.BatchNorm2d(new_channels),\n",
    "            nn.ReLU(inplace=True) if relu else nn.Sequential(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.primary_conv(x)\n",
    "        x2 = self.cheap_operation(x1)\n",
    "        out = torch.cat([x1,x2], dim=1)\n",
    "        return out[:,:self.oup,:,:]\n",
    "\n",
    "\n",
    "class GhostBottleneck(nn.Module):\n",
    "    \"\"\" Ghost bottleneck w/ optional SE\"\"\"\n",
    "\n",
    "    def __init__(self, in_chs, mid_chs, out_chs, dw_kernel_size=3,\n",
    "                 stride=1, act_layer=nn.ReLU, se_ratio=0.):\n",
    "        super(GhostBottleneck, self).__init__()\n",
    "        has_se = se_ratio is not None and se_ratio > 0.\n",
    "        self.stride = stride\n",
    "\n",
    "        # Point-wise expansion\n",
    "        self.ghost1 = GhostModule(in_chs, mid_chs, relu=True)\n",
    "\n",
    "        # Depth-wise convolution\n",
    "        if self.stride > 1:\n",
    "            self.conv_dw = nn.Conv2d(mid_chs, mid_chs, dw_kernel_size, stride=stride,\n",
    "                             padding=(dw_kernel_size-1)//2,\n",
    "                             groups=mid_chs, bias=False)\n",
    "            self.bn_dw = nn.BatchNorm2d(mid_chs)\n",
    "\n",
    "        # Squeeze-and-excitation\n",
    "        if has_se:\n",
    "            self.se = SqueezeExcite(mid_chs, se_ratio=se_ratio)\n",
    "        else:\n",
    "            self.se = None\n",
    "\n",
    "        # Point-wise linear projection\n",
    "        self.ghost2 = GhostModule(mid_chs, out_chs, relu=False)\n",
    "        \n",
    "        # shortcut\n",
    "        if (in_chs == out_chs and self.stride == 1):\n",
    "            self.shortcut = nn.Sequential()\n",
    "        else:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_chs, in_chs, dw_kernel_size, stride=stride,\n",
    "                       padding=(dw_kernel_size-1)//2, groups=in_chs, bias=False),\n",
    "                nn.BatchNorm2d(in_chs),\n",
    "                nn.Conv2d(in_chs, out_chs, 1, stride=1, padding=0, bias=False),\n",
    "                nn.BatchNorm2d(out_chs),\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        # 1st ghost bottleneck\n",
    "        x = self.ghost1(x)\n",
    "\n",
    "        # Depth-wise convolution\n",
    "        if self.stride > 1:\n",
    "            x = self.conv_dw(x)\n",
    "            x = self.bn_dw(x)\n",
    "\n",
    "        # Squeeze-and-excitation\n",
    "        if self.se is not None:\n",
    "            x = self.se(x)\n",
    "\n",
    "        # 2nd ghost bottleneck\n",
    "        x = self.ghost2(x)\n",
    "        \n",
    "        x += self.shortcut(residual)\n",
    "        return x\n",
    "\n",
    "class GhostNet(nn.Module):\n",
    "    def __init__(self, cfgs, num_classes=4, width=1.0, dropout=0.2):\n",
    "        super(GhostNet, self).__init__()\n",
    "        # setting of inverted residual blocks\n",
    "        self.cfgs = cfgs\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # building first layer\n",
    "        output_channel = _make_divisible(16 * width, 4)\n",
    "        self.conv_stem = nn.Conv2d(3, output_channel, 3, 2, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(output_channel)\n",
    "        self.act1 = nn.ReLU(inplace=True)\n",
    "        input_channel = output_channel\n",
    "\n",
    "        # building inverted residual blocks\n",
    "        stages = []\n",
    "        block = GhostBottleneck\n",
    "        for cfg in self.cfgs:\n",
    "            layers = []\n",
    "            for k, exp_size, c, se_ratio, s in cfg:\n",
    "                output_channel = _make_divisible(c * width, 4)\n",
    "                hidden_channel = _make_divisible(exp_size * width, 4)\n",
    "                layers.append(block(input_channel, hidden_channel, output_channel, k, s,\n",
    "                              se_ratio=se_ratio))\n",
    "                input_channel = output_channel\n",
    "            stages.append(nn.Sequential(*layers))\n",
    "\n",
    "        output_channel = _make_divisible(exp_size * width, 4)\n",
    "        stages.append(nn.Sequential(ConvBnAct(input_channel, output_channel, 1)))\n",
    "        input_channel = output_channel\n",
    "        \n",
    "        self.blocks = nn.Sequential(*stages)        \n",
    "\n",
    "        # building last several layers\n",
    "        output_channel = 640\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.conv_head = nn.Conv2d(input_channel, output_channel, 1, 1, 0, bias=True)\n",
    "        self.act2 = nn.ReLU(inplace=True)\n",
    "        self.classifier = nn.Linear(output_channel, num_classes)\n",
    "        self.act3 = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_stem(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = self.conv_head(x)\n",
    "        x = self.act2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        if self.dropout > 0.:\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.classifier(x)\n",
    "        x = self.act3(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "\n",
    "def ghostnet(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a GhostNet model\n",
    "    \"\"\"\n",
    "    cfgs = [\n",
    "        # k, t, c, SE, s \n",
    "        # stage1\n",
    "        [[3,  16,  16, 0, 1]],\n",
    "        # stage2\n",
    "        [[3,  48,  24, 0, 2]],\n",
    "        [[3,  72,  24, 0, 1]],\n",
    "        # stage3\n",
    "        [[5,  72,  40, 0.25, 2]],\n",
    "        [[5, 120,  40, 0.25, 1]],\n",
    "        # stage4\n",
    "        [[3, 240,  80, 0, 2]],\n",
    "        [[3, 200,  80, 0, 1],\n",
    "         [3, 184,  80, 0, 1],\n",
    "         [3, 184,  80, 0, 1],\n",
    "         [3, 480, 112, 0.25, 1],\n",
    "         [3, 672, 112, 0.25, 1]\n",
    "        ],\n",
    "        # stage5\n",
    "        [[5, 672, 160, 0.25, 2]],\n",
    "        [[5, 960, 160, 0, 1],\n",
    "         [5, 960, 160, 0.25, 1],\n",
    "         [5, 960, 160, 0, 1],\n",
    "         [5, 960, 160, 0.25, 1]\n",
    "        ]\n",
    "    ]\n",
    "    return GhostNet(cfgs, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "model = ghostnet()\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1, weight_decay=0.1)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g3REAbSEHZ1P",
    "outputId": "14f57556-715d-40c0-8ff3-31cd8f50a343"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2039, 0.2820, 0.2716, 0.2425]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input = torch.rand([1,3,256,256])\n",
    "# outputs = model(input)\n",
    "# outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oT7HxeNnHymg",
    "outputId": "464db301-3cde-4e9f-a454-11e9de3b8f01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GhostNet(\n",
       "  (conv_stem): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act1): ReLU(inplace=True)\n",
       "  (blocks): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): GhostBottleneck(\n",
       "        (ghost1): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
       "            (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (ghost2): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
       "            (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): GhostBottleneck(\n",
       "        (ghost1): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(16, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)\n",
       "        (bn_dw): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (ghost2): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=12, bias=False)\n",
       "            (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Conv2d(16, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): GhostBottleneck(\n",
       "        (ghost1): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(24, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=36, bias=False)\n",
       "            (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (ghost2): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=12, bias=False)\n",
       "            (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): GhostBottleneck(\n",
       "        (ghost1): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(24, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=36, bias=False)\n",
       "            (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv_dw): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
       "        (bn_dw): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (conv_reduce): Conv2d(72, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv_expand): Conv2d(20, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (ghost2): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(72, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)\n",
       "            (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=24, bias=False)\n",
       "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Conv2d(24, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): GhostBottleneck(\n",
       "        (ghost1): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(40, 60, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=60, bias=False)\n",
       "            (1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (conv_reduce): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv_expand): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (ghost2): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(120, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)\n",
       "            (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): GhostBottleneck(\n",
       "        (ghost1): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=120, bias=False)\n",
       "            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv_dw): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "        (bn_dw): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (ghost2): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=40, bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Conv2d(40, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): GhostBottleneck(\n",
       "        (ghost1): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(80, 100, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=100, bias=False)\n",
       "            (1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (ghost2): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(200, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "      (1): GhostBottleneck(\n",
       "        (ghost1): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(80, 92, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(92, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(92, 92, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=92, bias=False)\n",
       "            (1): BatchNorm2d(92, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (ghost2): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(184, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "      (2): GhostBottleneck(\n",
       "        (ghost1): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(80, 92, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(92, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(92, 92, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=92, bias=False)\n",
       "            (1): BatchNorm2d(92, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (ghost2): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(184, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "      (3): GhostBottleneck(\n",
       "        (ghost1): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (conv_reduce): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv_expand): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (ghost2): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(480, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(56, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=56, bias=False)\n",
       "            (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Conv2d(80, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (4): GhostBottleneck(\n",
       "        (ghost1): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(112, 336, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(336, 336, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=336, bias=False)\n",
       "            (1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (conv_reduce): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv_expand): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (ghost2): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(672, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(56, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=56, bias=False)\n",
       "            (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): GhostBottleneck(\n",
       "        (ghost1): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(112, 336, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(336, 336, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=336, bias=False)\n",
       "            (1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "        (bn_dw): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (conv_reduce): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv_expand): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (ghost2): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(672, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(112, 112, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=112, bias=False)\n",
       "          (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Conv2d(112, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): GhostBottleneck(\n",
       "        (ghost1): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(160, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (ghost2): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(960, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "      (1): GhostBottleneck(\n",
       "        (ghost1): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(160, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (conv_reduce): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv_expand): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (ghost2): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(960, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "      (2): GhostBottleneck(\n",
       "        (ghost1): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(160, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (ghost2): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(960, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "      (3): GhostBottleneck(\n",
       "        (ghost1): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(160, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (conv_reduce): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv_expand): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (ghost2): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(960, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (9): Sequential(\n",
       "      (0): ConvBnAct(\n",
       "        (conv): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (conv_head): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (act2): ReLU(inplace=True)\n",
       "  (classifier): Linear(in_features=640, out_features=4, bias=True)\n",
       "  (act3): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "PnjCgslFHyrt"
   },
   "outputs": [],
   "source": [
    "\n",
    "#added new parameter of start_epoch\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs,resume=None,model_name='ghosnet'):\n",
    "    since = time.time()\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "\n",
    "    best_acc = 0.0\n",
    "    start_epoch = 0\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'validation']:#\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            ground_truth = []\n",
    "            predictions = []\n",
    "            confidence = []\n",
    "\n",
    "            new_live_images = []\n",
    "            new_ground_truth = []\n",
    "            new_predictions = []\n",
    "\n",
    "            \n",
    "            # Iterate over data.\n",
    "            temp = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "\n",
    "                    _, preds = torch.max(outputs, dim=1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    else:\n",
    "                        ground_truth.append(labels)\n",
    "                        predictions.append(preds)\n",
    "                        # print(outputs)\n",
    "                        conf_scores,idx = torch.max(outputs,dim=1)\n",
    "                        # print(idx)\n",
    "                        confidence.append(conf_scores)\n",
    "\n",
    "                \n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                \n",
    "            if phase == 'validation':\n",
    "                scheduler.step(running_loss)\n",
    "           \n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            if phase == \"train\":\n",
    "              train_loss.append(epoch_loss)\n",
    "            elif phase == \"test\":\n",
    "              test_loss.append(epoch_loss)\n",
    "            else:\n",
    "              val_loss.append(epoch_loss)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'validation' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "\n",
    "                model_weight_dir = './weights_'+model_name\n",
    "\n",
    "        print()\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    return model, train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "szMxTkmZHyt4"
   },
   "outputs": [],
   "source": [
    "model_training = train_model(model, criterion, \n",
    "                              optimizer,scheduler,\n",
    "                              num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GIZSoaDqHZ3H"
   },
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "plt.plot(model_training[1], label=\"Training set\")\n",
    "plt.plot(model_training[2], label=\"Validation set\")\n",
    "#training = mpatches.Patch(color='skyblue', label='Training')\n",
    "#validation = mpatches.Patch(color='orange', label='validation')\n",
    "plt.xlabel(\"No. of epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "327kUmKAHZ5n",
    "outputId": "4c25d6cf-08a4-43a4-fc0e-40e44972dc60"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "test_dataset = datasets.ImageFolder(\"/content/drive/MyDrive/GNR/dataset/test/\", data_transforms['test'])\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "                                    test_dataset,                                        \n",
    "                                    batch_size=1,\n",
    "                                    shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21YdR8ZuHZ8I"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    # l=[]\n",
    "    # p=[]\n",
    "    for data in tqdm(test_dataloader):\n",
    "        images, labels = data\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        # l.append(labels)\n",
    "        # p.append(predicted)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 100 test images: %d %%' % (\n",
    "    100 * correct / total))\n",
    "# print(l)\n",
    "# print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pUyKTNsEHaA3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GNR_Project",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
